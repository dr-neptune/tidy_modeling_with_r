#+BEGIN_SRC R
library(tidyverse)
library(magrittr)
library(zeallot)
library(tidymodels)
library(modeldata)
#+END_SRC

#+TITLE: The Ames Housing Data

#+BEGIN_SRC R
data(ames)

ames %>% glimpse
#+END_SRC

#+RESULTS:

* Exploring Important Features

#+BEGIN_SRC R
ames %>%
    ggplot(aes(x = Sale_Price)) +
    geom_histogram(bins = 50)
#+END_SRC


The data is right skewed -- there are more inexpensive houses than expensive ones.

When modeling this outcome, a strong argument can be made that the price should be log-transformed. The advantages of doing this are that no houses would be predicted with negative sales prices and that errors in predicting expensive houses will not have undue influence on the model

#+BEGIN_SRC R
ames %>%
    ggplot(aes(x = Sale_Price)) +
    geom_histogram(bins = 50) +
    scale_x_log10()

ames %<>% mutate(Sale_Price = log10(Sale_Price))
#+END_SRC

It is critical to conduct EDA prior to beginning any modeling.

Some basic questions:

- Are there any odd or noticeable things about the distributions of the individual predictors? Is there much skewness or any pathological distributions?
- Are there high correlations between predictors?
- Are there associations between predictors and the outcomes?

#+TITLE: Spending Our Data

* Common Methods for Splitting Data

#+BEGIN_SRC R
set.seed(8888)

(ames_split <- initial_split(ames, prop = 0.8))

c(ames_train, ames_test) %<-% list("training" = training(ames_split),
                                   "testing" = testing(ames_split))
#+END_SRC

Simple random sampling is appropriate in many cases, but there are exceptions. When there is a dramatic class imbalance in classification problems, using a simple random sample may haphazardly allocate these infrequent samples disproportionately into the training or test set.

To avoid this, /statified sampling/ can be used. The training / test split is conducted separately within each class and then these subsamples are combined into the overall training and test set. For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling conducted four separate times.

We could run a stratified sample with the strat argument:

#+BEGIN_SRC R
(ames_split <- initial_split(ames, prop = 0.8, strata = Sale_Price))

c(ames_train, ames_test) %<-% list("training" = training(ames_split),
                                   "testing" = testing(ames_split))
#+END_SRC

* Multi-Level Data

  - For longitudinal data, the same independent experimental unit can be measured over multiple time points. An example would be a human subject in a medical trial
  - A batch of manufactured product might also be considered the independent experimental unit. In repeated measures designs, replicate data points from a batch are collected

#+TITLE: Feature Engineering with Recipes

Feature engineering encompasses activities that reformat predictor values to make them easier for a model to use effectively. 

Some examples:

- Correlation between predictors can be reduced via feature extraction or the removal of some predictors
- When some predictors have missing values, they can be imputed using a submodel
- Models that use variance-type measures may benefit from coercing the distribution of some skewed predictors to be symmetric by estimating a transformation

Feature engineering and data processing can also involve reformatting required by the model. Some models use geometric distance metrics, and consequently, numeric predictors should be centered and scaled so that they are all in the same units. Otherwise, the distance values would be biased by the scale of each column.

* A Simple Recipe for the Ames Housing Data

In this section, we will focus on a small subset of the predictors available in the Ames housing data:

- The neighborhood
- The general living area
- The year built
- The type of building

#+BEGIN_SRC R
(lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames))
#+END_SRC

A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them.

#+BEGIN_SRC R
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
       data = ames_train) %>%
    step_log(Gr_Liv_Area, base = 10) %>%
    step_dummy(all_nominal()) -> simple_ames
#+END_SRC

This templating is nice, because

- These computations can be recycled across models since they are not tightly coupled to the modeling function
- A recipe enables a broader set of data processing choices than formulas can offer
- The syntax can be very compact (all_* (numeric, predictors, outcomes))
- All data processing can be captured in a single R object instead of in scripts that are repeated, or even spread across different files

* Using Recipes

The second phase for using a recipe is to estimate any quantities required by the steps using the prep() function. For example, we can use step_normalize() to center and scale any predictors selected in the step.

#+BEGIN_SRC R
simple_ames %<>% prep()
#+END_SRC

When retain = TRUE (the default), the prepped version of the training set is kept within the recipe.
Since prep() has to execute the recipe as it proceeds, it may be advantageous to keep this version of the training set so that, if the dataset is to be used later, redundant calculations may be avoided. If the training set is large however, it may be problematic to keep such a large amount of data in memory, and we should use retain = FALSE for this.

The third phase of recipe usage is to apply the preprocessing operations to a dataset using the bake() function. The bake() function can apply the recipe to any data set.

#+BEGIN_SRC R
simple_ames %>%
    ## to use the test set, this would be the syntax
    bake(new_data = ames_test) %>%
    names() %>%
    head()

## bake can also take selectors
simple_ames %>%
    bake(ames_test, starts_with("Neighborhood_"))

## to avoid repeating calculations, we can set new_data = NULL to quickly return the training set 
simple_ames %>%
    bake(new_data = NULL) %>% nrow() -> 
#+END_SRC

A recipe is a 3 phase process summarized as:

recipe() -> prep() -> bake()

recipe() defines the preprocessing
prep() calculates statistics
bake() applies the preprocessing to data sets

* Encoding Qualitative Data in a Numeric Format

One of the most common feature engineering tasks is transforming nominal or qualitative data (factors or characters)

- step_unknown() can be used to change missing values to a dedicated factor level
- step_novel() can allot a new level for factors encountered in future data 

#+BEGIN_SRC R
ames_train %>%
    ggplot(aes(y = Neighborhood)) +
    geom_bar() +
    labs(y = NULL) 
#+END_SRC

Here there are 2 neighborhoods that have less than 5 properties in the training data; in this case, no houses at all in the landmark neighborhood were included in the training set. 

For some models, it may be problematic to have dummy variables with a single non-zero entry in the column. At a minimum, it is highly improbable that those features would be important to a model. 

If we add step_other(Neighborhood, threshold = 0.01) to our recipe, the bottom 1% of neighborhoodsd will be lumped into a new level called other.

#+BEGIN_SRC R
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
       data = ames_train) %>%
    step_log(Gr_Liv_Area, base = 10) %>%
    step_other(Neighborhood, threshold = 0.01) %>% 
    step_dummy(all_nominal()) -> simple_ames
#+END_SRC

There are a few strategies for converting a factor predictor to a numeric format. 

The most common is to use dummy, or indicator variables. This is traditionally called the one-hot encoding and can be achieved using the one_hot argument to step_dummy()

Traditional dummy variables require that all of the possible categories be known to create a full set of numeric features. There are other methods for doing this transformation to a numeric format. *Feature hashing* methods only consider the value of the category to assign it to a predefined pool of dummy variables. This can be a good strategy when there are a large number of possible categories, but the statistical properties may not be optimal. 

Another method that is useful when there are a large number of categories is called effect or likelihood encodings. This method replaces the original data with a single numeric column that measures the /effect/ of that data. For example, with the neighborhood predictor, the mean sale price is computed for each neighborhood and these means are substituted for the original data values. This can be effective, but should be used with care. In effect, a mini-model is being added to the actual model and this can lead to overfitting. 

* Interaction Terms

Interaction effects involve two or more predictors. Such an effect occurs when one predictor has an effect on the outcome that is contingent on one or more predictors. 

After exploring the Ames training set, we might find that the regression slopes for the general living area differ for different building types.

#+BEGIN_SRC R
ames_train %>%
    ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
    geom_point(alpha = 0.2) +
    facet_wrap(~ Bldg_Type) +
    geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "blue") +
    scale_x_log10() +
    scale_y_log10() +
    labs(x = "General Living Area",
         y = "Sale Price (USD)")
#+END_SRC

In base R, we would define interactions as such:

#+BEGIN_SRC R
Sale_Price ~ Neighborhood + log10(Gr_Liv_Area):Bldg_Type

Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type
#+END_SRC

where * expands those columns to the main effects and interaction term.

We could also add the interaction effects in a recipe:

#+BEGIN_SRC R
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
       data = ames_train) %>%
    step_log(Gr_Liv_Area, base = 10) %>%
    step_other(Neighborhood, threshold = 0.01) %>% 
    step_dummy(all_nominal()) %>%
    step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) -> simple_ames 
#+END_SRC


* Skipping Steps for New Data

For simple transformations of the outcome column(s), it is strongly suggested that those operations be conducted outside of the recipe.

* Other Examples of Recipe Steps

** Spline Functions

When a predictor has a nonlinear relationship with the outcome, some types of predictive models can adaptively approximate this relationship during training. 
However, simpler is usually better and it is common to try to use a simple model like a linear fit to add in specific nonlinear features for predictors that make need them.



